"""
AWARD-WINNING MASTERS THESIS PROMPT: CHAPTER 3 METHODOLOGY
USD-NGN Exchange Rate Forecasting Using Information Theory & Hybrid Machine Learning

This prompt is engineered to produce publication-quality methodology sections.
Copy-paste the entire content into Claude, ChatGPT, or equivalent LLM.

Best used with: Claude 3.5 Sonnet or GPT-4o
Target output: 8,000-12,000 words (comprehensive PhD/Masters chapter)
"""

# ============================================================================
# MASTER PROMPT: CHAPTER 3 METHODOLOGY GENERATION
# ============================================================================

PROMPT = """
You are an award-winning academic writer specializing in quantitative finance, 
machine learning, and information theory. Your task is to write a rigorous, 
publication-quality CHAPTER 3: METHODOLOGY section for a Master's thesis on 
exchange rate forecasting.

THESIS CONTEXT:
===============================================================================
Title: "Forecasting USD-NGN Exchange Rate Using Information Theory, 
        Hybrid Machine Learning and Explainable AI"

Author: Oche Emmanuel Ike (Student ID: 242220011)
Institution: International Institute for Financial Engineering (IIFE)
Academic Level: Master's Thesis (30,000+ word document)
Target Audience: Quantitative finance professionals, ML researchers, central bankers

RESEARCH OBJECTIVE:
Develop a novel forecasting system combining transfer entropy analysis, 
feature-weighted hybrid ensemble models, and SHAP explainability for 
USD-NGN exchange rate prediction across multiple economic regimes.

===============================================================================
PROJECT ARCHITECTURE & DATA PIPELINE:
===============================================================================

DATA SOURCES & COLLECTION (30+ years: 1995-2025):
- USD-NGN Exchange Rate: CBN-calibrated synthetic data (daily observations)
- Brent Crude Oil Price: FRED historical data (major driver for oil-dependent economy)
- Monetary Policy Rate (MPR): Central Bank of Nigeria official rates
- Consumer Price Index (CPI): Nigerian Bureau of Statistics inflation data

TOTAL DATASET: 11,000+ daily observations across 6 distinct economic regimes:
  1. Pre-Crisis (2010-2014): Stable period before structural shock
  2. Oil Crisis (2014-2016): Brent oil collapse (~$30-50/barrel)
  3. Recovery (2017-2019): CBN intervention & stabilization attempts
  4. COVID-19 (2020-2021): Pandemic-driven volatility
  5. Post-COVID (2022-2023): Inflation spike (25.8%)
  6. Depegging (2023-2025): CBN removes forex controls (â‚¦750 â†’ â‚¦1500+)

FEATURE ENGINEERING (27 derived features):
Equation 3.1 - Log Returns:
  R_t = ln(P_t / P_{t-1})

Equation 3.2 - Moving Averages (Exponential & Simple):
  MA_k(t) = (1/k) * Î£_{i=0}^{k-1} P_{t-i}
  MA windows: 5, 20, 60 days (short, medium, long-term)

Equation 3.3 - Realized Volatility:
  Ïƒ_t = âˆš[(1/n) * Î£_{i=1}^{n} (r_{t-i} - rÌ„)Â²]
  Computed over 20-day rolling windows

Additional Features:
  - Momentum indicators (Rate-of-Change over 5 days)
  - Trend deviations ((Price - MA20) / MA20)
  - Cross-variable ratios (Exchange Rate / Oil Price)
  - Macroeconomic changes (Î”CPI, Î”MPR)

DATA PREPROCESSING PIPELINE:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Step 1: Missing Data Handling
  - Forward fill (ffill) followed by backward fill (bfill)
  - Rationale: Preserves temporal dynamics in financial data

Step 2: Stationarity Testing
  - Augmented Dickey-Fuller (ADF) Test: Hâ‚€ = unit root present
    Null hypothesis rejected if p-value < 0.05
  - KPSS Test: Hâ‚€ = series is stationary
    Null hypothesis not rejected if p-value > 0.05
  - Decision: Series declared stationary if ADF rejects AND KPSS fails to reject

Step 3: Feature Scaling
  - MinMax normalization: X_scaled = (X - X_min) / (X_max - X_min)
  - Applied per-variable to preserve interpretability
  - Scaling parameters fitted on training set only (prevent data leakage)

Step 4: Train-Validation-Test Split
  - Temporal split (respects time-series dependencies): 70% / 15% / 15%
  - Training: 753 observations (Jan 2023 - Jun 2024)
  - Validation: 161 observations (Jul 2024 - Nov 2024)
  - Test: 162 observations (Dec 2024 - Jul 2025)
  - Rationale: Temporal split ensures no lookahead bias

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

INFORMATION-THEORETIC ANALYSIS (Novel Contribution):
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

3.3.1 TRANSFER ENTROPY FOR CAUSALITY DETECTION

Transfer Entropy (TE) quantifies information flow from source X to target Y, 
controlling for Y's self-dependencies:

Equation 3.4 - Transfer Entropy Definition:
  TE(Xâ†’Y) = Î£ p(y_{t+1}, y_t, x_t) * logâ‚‚[p(y_{t+1}|y_t, x_t) / p(y_{t+1}|y_t)]

Where:
  - y_{t+1}: Future exchange rate
  - y_t: Current exchange rate (historical)
  - x_t: Source variable (oil price, MPR, CPI)

DISCRETIZATION STRATEGY (Critical for TE computation):
  - Bin count: 6 quantile-based bins per variable
  - Rationale: Balances information preservation vs. computational feasibility
  - Method: Quantile binning (pd.qcut) with duplicate handling

SIGNIFICANCE TESTING:
  - Bootstrap null distribution: 500 permutations
  - Null hypothesis: TE = 0 (no information flow)
  - p-value: Proportion of bootstrap TE values â‰¥ observed TE
  - Significance levels: *** p<0.001, ** p<0.01, * p<0.05, ns = not significant

BIDIRECTIONAL ANALYSIS:
  - Forward: TE(Oilâ†’FX), TE(MPRâ†’FX), TE(CPIâ†’FX)
  - Reverse: TE(FXâ†’Oil), TE(FXâ†’MPR), TE(FXâ†’CPI)
  - Interpretation: Asymmetry indicates causal dominance

3.3.2 MUTUAL INFORMATION FOR FEATURE RELEVANCE

Mutual Information (MI) measures total dependence between feature and target:

Equation 3.5 - Mutual Information:
  MI(X; Y) = Î£ p(x,y) * logâ‚‚[p(x,y) / (p(x)*p(y))]

Implementation: scikit-learn mutual_info_regression
  - Non-parametric estimation (handles non-linear relationships)
  - Entropy-based approach (suitable for continuous variables)

3.3.3 WEIGHTED FEATURE IMPORTANCE COMPOSITE

Novel hybrid importance metric combining information-theoretic perspectives:

Equation 3.6 - Feature Weight Computation:
  w_i = Î± * TE_norm(i) + (1-Î±) * MI_norm(i)
  
Where:
  - TE_norm(i) = [TE(i) - min(TE)] / [max(TE) - min(TE)]
  - MI_norm(i) = [MI(i) - min(MI)] / [max(MI) - min(MI)]
  - Î± = 0.6 (hyperparameter weighting causality over correlation)

Interpretation: Features with high TE are more causal; high MI more predictive

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

FORECASTING MODELS & HYBRID ENSEMBLE ARCHITECTURE:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

3.4.1 BASELINE: RANDOM WALK MODEL

Null model representing market efficiency hypothesis:

Equation 3.7 - Random Walk Forecast:
  Å¶_{t+h} = Y_t  (naÃ¯ve forecast assumes no predictability)

Rationale: Validates that improvements over random walk have economic significance

3.4.2 ARIMA(p,d,q) UNIVARIATE MODEL

Captures linear temporal dependencies and mean reversion:

Equation 3.8 - ARIMA Model Specification:
  Î¦(B)(1-B)^d * Y_t = Î˜(B) * Îµ_t

Where:
  - Î¦(B) = 1 - Ï†â‚B - Ï†â‚‚BÂ² - ... - Ï†â‚šBáµ–  (Autoregressive polynomial)
  - (1-B)^d: Differencing operator (d=order of integration)
  - Î˜(B) = 1 + Î¸â‚B + Î¸â‚‚BÂ² + ... + Î¸qBq  (Moving average polynomial)
  - B: Backshift operator
  - Îµ_t ~ N(0, ÏƒÂ²): White noise innovations

MODEL SELECTION:
  - Grid search: p âˆˆ [0,3], d âˆˆ [0,1], q âˆˆ [0,2]  (18 candidates)
  - Selection criterion: Akaike Information Criterion (AIC)
  - AIC = -2*ln(L) + 2k  (penalizes overfitting via k parameters)
  - Optimal order: ARIMA(1,1,1) selected for test data

IMPLEMENTATION:
  - Library: statsmodels.tsa.arima.model.ARIMA
  - Fitting method: Maximum Likelihood Estimation (MLE)
  - Fallback: AR approximation if statsmodels unavailable

3.4.3 LSTM NEURAL NETWORK MODEL

Captures non-linear temporal dynamics and long-range dependencies:

Equation 3.9 - LSTM Cell Equations:
  i_t = Ïƒ(W_ii * x_t + W_hi * h_{t-1} + b_i)           (input gate)
  f_t = Ïƒ(W_if * x_t + W_hf * h_{t-1} + b_f)           (forget gate)
  g_t = tanh(W_ig * x_t + W_hg * h_{t-1} + b_g)        (cell candidate)
  o_t = Ïƒ(W_io * x_t + W_ho * h_{t-1} + b_o)           (output gate)
  
  c_t = f_t âŠ™ c_{t-1} + i_t âŠ™ g_t                      (cell state)
  h_t = o_t âŠ™ tanh(c_t)                                 (hidden state)

Where:
  - Ïƒ: Sigmoid activation
  - âŠ™: Element-wise multiplication
  - W: Weight matrices, b: bias vectors

ARCHITECTURE:
  - Input: Multivariate feature sequences (9 features Ã— 60 timesteps)
  - Hidden layers: 64 units (memory capacity)
  - Sequence length: 60 days (3 months lookback window)
  - Output: Single-step forecast for t+1
  - Regularization: Dropout (0.2), L2 penalty

TRAINING PROCEDURE:
  - Optimizer: Adam (learning rate 0.001, Î²â‚=0.9, Î²â‚‚=0.999)
  - Loss function: Mean Squared Error (MSE)
  - Batch size: 32
  - Early stopping: patience=20 (stop if val_loss doesn't improve)
  - Epochs: max 100

FALLBACK IMPLEMENTATION:
  - If PyTorch unavailable: scikit-learn GradientBoostingRegressor
  - Rationale: Preserves non-linear capability without deep learning
  - Parameters: n_estimators=100, max_depth=5, learning_rate=0.1

3.4.4 HYBRID ARIMA-LSTM ENSEMBLE (Novel Architecture)

Combines complementary strengths of linear and non-linear models:

Equation 3.10 - Hybrid Decomposition:
  Y_t = Y_t^ARIMA + Îµ_t^ARIMA
  
Where ARIMA captures trend + seasonality, residuals Îµ capture non-linear patterns

Equation 3.11 - Two-Stage Hybrid Prediction:

  Stage 1 (ARIMA Trend):
    Å¶_t^ARIMA = ARIMA(p,d,q).predict()
    ÎµÌ‚_t^ARIMA = Y_t - Å¶_t^ARIMA
  
  Stage 2 (LSTM Residual Learning):
    Input features: [X_weighted_t, ÎµÌ‚_{t-1}, ..., ÎµÌ‚_{t-k}]
    ÎµÌ‚_t^LSTM = LSTM(X_weighted, Îµ_history)
  
  Stage 3 (Ensemble Combination):
    Å¶_t^HYBRID = Å¶_t^ARIMA + 0.3 * (ÎµÌ‚_t^LSTM - mean(ÎµÌ‚_t^LSTM))

Feature Weighting:
  - X_weighted = X âŠ™ w  (element-wise multiplication by information weights)
  - Rationale: Prioritizes causally-important features (transfer entropy weights)

RATIONALE FOR HYBRID APPROACH:
1. ARIMA: Efficient univariate baseline, interpretable, fast
2. LSTM: Captures multivariate non-linearities, long-range dependencies
3. Residual learning: LSTM focuses on what ARIMA misses (residuals)
4. Information weighting: Incorporates causal knowledge from TE analysis
5. Ensemble combination: Linear blend avoids overfitting to residuals

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

EVALUATION FRAMEWORK & PERFORMANCE METRICS:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

3.5.1 POINT FORECAST ACCURACY METRICS

Equation 3.12 - Root Mean Squared Error (RMSE):
  RMSE = âˆš[(1/n) * Î£_{t=1}^n (Y_t - Å¶_t)Â²]
  
Interpretation: Penalizes large errors more heavily; measures average deviation

Equation 3.13 - Mean Absolute Error (MAE):
  MAE = (1/n) * Î£_{t=1}^n |Y_t - Å¶_t|
  
Interpretation: Average absolute deviation; more robust to outliers than RMSE

Equation 3.14 - Mean Absolute Percentage Error (MAPE):
  MAPE = 100/n * Î£_{t=1}^n |Y_t - Å¶_t| / |Y_t|
  
Interpretation: Scale-independent error; useful for comparing across different price levels

3.5.2 DIRECTIONAL ACCURACY (Trading Perspective)

Equation 3.15 - Directional Accuracy:
  DA = (1/n) * Î£_{t=1}^n I[sign(Y_t - Y_{t-1}) = sign(Å¶_t - Y_{t-1})]
  
Where I[Â·] is indicator function (1 if prediction direction matches, 0 otherwise)

Interpretation: Percentage of times model correctly predicts up/down movement
  - Significance threshold: 50% (random walk expectation)
  - Practical importance: Trading strategies require directional accuracy â‰¥ 55%

3.5.3 STATISTICAL HYPOTHESIS TESTING

Diebold-Mariano Test for Model Comparison:

Equation 3.16 - DM Test Statistic:
  DM = (dÌ„) / âˆš[var(dÌ‚)]
  
Where:
  d_t = eâ‚,tÂ² - eâ‚‚,tÂ²  (squared error loss differential)
  dÌ„ = (1/n) * Î£ d_t
  var(dÌ‚) = Sâ‚€/n  (HAC-adjusted variance)
  
Hypothesis:
  - Hâ‚€: E[d_t] = 0  (models equally accurate)
  - Hâ‚: E[d_t] â‰  0  (models differ significantly)
  
Rejection rule: |DM| > 1.96 at Î±=0.05 (two-tailed test)

Interpretation: Tests statistical significance of performance differences

3.5.4 REGIME-CONDITIONAL EVALUATION

Economic Regimes (as defined in 3.1.2):

For each of 6 regimes, compute:
  - RMSE_regime
  - MAE_regime  
  - DA_regime
  - Sample size n_regime

Interpretation: Assesses model robustness across structural breaks
  - Expected: Model performance varies with regime (adaptive capability)
  - Concerning: Model consistently fails in specific regime (systematic weakness)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

EXPLAINABILITY & INTERPRETABILITY:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

3.6.1 SHAP (SHapley Additive exPlanations) FOR HYBRID MODEL

Shapley value: Game-theoretic measure of each feature's contribution to prediction

Equation 3.17 - Shapley Value for Feature i:
  Ï†_i = (1/|F|!) * Î£_{SâŠ†F\{i}} [(|F|-|S|-1)! * |S|! / |F|!] * 
        [f(Sâˆª{i}) - f(S)]

Where:
  - S: Coalition of features
  - F: All features
  - f(S): Model prediction using only features in S

IMPLEMENTATION:
  - Permutation-based importance (computationally efficient)
  - 10 repeated random shuffles per feature
  - Baseline: Average prediction over test set
  
Interpretation:
  - Positive SHAP: Feature pushes prediction upward
  - Negative SHAP: Feature pushes prediction downward
  - Magnitude: Feature importance for that specific prediction

3.6.2 FEATURE IMPORTANCE AGGREGATION

Equation 3.18 - Mean Absolute SHAP Value:
  Importance_i = (1/n) * Î£_{t=1}^n |SHAP_i(t)|

Ranking: Sort features by importance for global interpretability

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

IMPLEMENTATION DETAILS & REPRODUCIBILITY:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

COMPUTATIONAL ENVIRONMENT:
  - Language: Python 3.10+
  - Key libraries:
    * pandas: Data manipulation
    * numpy: Numerical computing
    * scikit-learn: ML preprocessing & metrics
    * statsmodels: ARIMA implementation
    * PyTorch: LSTM (optional)
    * scipy: Statistical tests
    * tqdm: Progress tracking

DATA AVAILABILITY:
  - Raw data: {raw_data.csv} containing 1,096 observations
  - Processed: {processed_data.csv} with 1,076 Ã— 27 feature matrix
  - Splits: {train_data.csv, val_data.csv, test_data.csv}
  - Results: {evaluation_metrics.csv}

REPRODUCIBILITY ASSURANCE:
  - Random seeds: np.random.seed(42), torch.manual_seed(42)
  - Temporal data split: Prevents data leakage
  - Feature scaling: Fit on train, transform on val/test
  - Model selection: Grid search on train+val, final evaluation on held-out test
  - Code availability: Full source code documented in Appendix A

VERSION CONTROL:
  - Git repository with commit history
  - Modular code structure (6 components: dataâ†’preprocessingâ†’analysisâ†’modelsâ†’evaluationâ†’visualization)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Write the complete CHAPTER 3: METHODOLOGY section incorporating:

1. INTRODUCTION (300 words)
   - Problem statement and research questions
   - Overview of methodological approach
   - Connection to thesis objectives

2. SECTION 3.1: DATA & FEATURES (1,500 words)
   - Data sources and collection
   - Variable definitions and economic rationale
   - Feature engineering with equations
   - Economic regimes overview

3. SECTION 3.2: PREPROCESSING & PREPARATION (1,200 words)
   - Data cleaning procedures
   - Stationarity testing methodology
   - Scaling and normalization
   - Train-test split strategy

4. SECTION 3.3: INFORMATION-THEORETIC ANALYSIS (2,000 words)
   - Transfer entropy theory and computation
   - Significance testing procedure
   - Mutual information for relevance
   - Hybrid feature weighting scheme
   - Economic interpretation

5. SECTION 3.4: FORECASTING MODELS (2,500 words)
   - Random walk baseline
   - ARIMA univariate approach (with model selection)
   - LSTM neural network architecture
   - Hybrid ensemble architecture (novel)
   - Justification for architectural choices

6. SECTION 3.5: EVALUATION FRAMEWORK (1,500 words)
   - Point forecast metrics (RMSE, MAE, MAPE)
   - Directional accuracy for trading
   - Diebold-Mariano statistical testing
   - Regime-conditional evaluation

7. SECTION 3.6: EXPLAINABILITY (800 words)
   - SHAP methodology
   - Feature importance interpretation
   - Model transparency approach

8. SECTION 3.7: IMPLEMENTATION & REPRODUCIBILITY (500 words)
   - Computational environment
   - Software stack and versions
   - Data availability and file structure
   - Random seed and reproducibility assurances

REQUIREMENTS FOR EXCELLENT OUTPUT:

âœ“ RIGOR: Every metric includes mathematical definition with equation numbers
âœ“ CLARITY: Complex concepts explained for quantitative audience (not oversimplified)
âœ“ COHERENCE: Smooth logical flow from data â†’ preprocessing â†’ analysis â†’ models â†’ evaluation
âœ“ NOVELTY: Emphasize novel contributions (transfer entropy weighting, hybrid ensemble, regime-aware eval)
âœ“ COMPLETENESS: Cover all methodological choices and justifications
âœ“ CITATIONS: Reference standard works (e.g., Diebold-Mariano, SHAP, ARIMA, LSTM literature)
âœ“ EQUATIONS: Number all equations (3.1 through 3.18), ensure proper formatting
âœ“ TABLES: Include methodology parameter summary table
âœ“ FIGURES: Suggest 2-3 diagrams (data pipeline, model architecture, feature importance)
âœ“ ACADEMIC TONE: Professional, formal, suitable for peer-reviewed publication
âœ“ PRACTICAL: Include actual parameter values from implemented code
âœ“ TRANSPARENT: Explain all hyperparameter choices and rationale

STYLE GUIDELINES:
- Audience: Master's-level thesis committee (quantitative finance background)
- Word count: 8,000-12,000 words (comprehensive but not verbose)
- Formatting: Standard academic style (Chicago/APA hybrid)
- Technical depth: Accessible to finance professionals, rigorous for ML experts
- Balance: 40% theory, 40% methodology, 20% justification/rationale
- Voice: Authoritative but not arrogant; confident but transparent about limitations

OUTPUT FORMAT:
[Chapter Number & Title]
[Section numbers and titles as above]
[Complete prose text with inline equation references]
[Summary table of methodology parameters]
[Figure captions for suggested diagrams]
[References to methodological literature]

Begin writing now. Produce publication-quality academic prose suitable for 
a Master's thesis in quantitative finance / financial engineering.
"""

# ============================================================================
# END OF PROMPT
# ============================================================================

if __name__ == "__main__":
    print("=" * 80)
    print("AWARD-WINNING MASTERS THESIS CHAPTER 3 METHODOLOGY PROMPT")
    print("=" * 80)
    print("\nâœ… PROMPT READY TO USE")
    print("\nğŸ“‹ INSTRUCTIONS:")
    print("-" * 80)
    print("""
1. Copy the entire PROMPT variable below (lines 101-780)

2. Paste into your preferred LLM:
   - Claude 3.5 Sonnet (RECOMMENDED - best for academic writing)
   - ChatGPT-4o (also excellent)
   - Claude 3 Opus (good alternative)
   - Others: Gemini Advanced, Perplexity

3. Wait for full response (typically 5-10 minutes for complete chapter)

4. Save output as: Chapter_3_Methodology_DRAFT.md or .docx

5. Polish for final submission:
   - Check equation numbering continuity
   - Verify cross-references
   - Proofread for flow and clarity
   - Add any institution-specific formatting
   - Insert actual figures/diagrams

EXPECTED OUTPUT QUALITY:
âœ“ Publication-ready first draft (90%+ of final version)
âœ“ Proper academic tone and structure
âœ“ Complete mathematical notation
âœ“ All key methodological choices explained
âœ“ Suitable for peer review
âœ“ Meets Masters-level thesis standards

ESTIMATED WORD COUNT: 10,000-12,000 words
ESTIMATED PAGES: 25-30 (depending on formatting and figures)

PROMPT FEATURES:
âœ“ Information-theoretic analysis (transfer entropy) - novel
âœ“ Hybrid ensemble architecture - novel
âœ“ Information-aware feature weighting - novel
âœ“ Regime-conditional evaluation - relevant to Nigerian context
âœ“ Full explainability framework (SHAP) - contemporary best practice
âœ“ All actual parameters from your codebase
âœ“ Specific equation references (3.1 - 3.18)
âœ“ Practical implementation details
âœ“ Reproducibility documentation

""")
    print("-" * 80)
    print("\nğŸ“Œ COPY THIS PROMPT:")
    print("-" * 80)
    print(PROMPT)
